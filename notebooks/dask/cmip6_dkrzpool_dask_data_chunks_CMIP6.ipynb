{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Chunking with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate:\n",
    "\n",
    "* Xarray + Dask\n",
    "* NetCDF file Chunks versus Dask Chunks\n",
    "* chunk shapes\n",
    "\n",
    "The following material uses Coupled Model Intercomparison Project (CMIP6) collections. Please see the data collection [catalogue](https://esgf-data.dkrz.de) and [CMIP6 terms of use](https://pcmdi.llnl.gov/CMIP6/TermsOfUse/TermsOfUse6-1.html) for more information. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slightly adapted version from: https://github.com/NCI-data-analysis-platform/climate-cmip \n",
    "- Original Authors: NCI Virtual Research Environment Team\n",
    "- Keywords: CMIP6, Xarray, Dask, Chunks\n",
    "- Creation Date: 2019-June; Updated: 2020-May\n",
    "---\n",
    "- Adaptation for DKRZ data pool: S. Kindermann, August 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the correct status link for dashboard\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "dask.config.config.get('distributed').get('dashboard').update({'link':'{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this notebook on your local computer or NCI's VDI instance, you can create cluster\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We will use precipitation data from SSP5-85 from the ACCESS-CM2 model in this example. Let's take a look at some of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On HPC system: netcdf module must be loaded\n",
    "\n",
    "!ncdump -hst '/pool/data/CMIP6/data/ScenarioMIP/CSIRO-ARCCSS/ACCESS-CM2/ssp585/r1i1p1f1/day/pr/gn/v20191108/pr_day_ACCESS-CM2_ssp585_r1i1p1f1_gn_20150101-20641231.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outside of HPC system, access via THREDDS\n",
    "\n",
    "!ncdump -hst 'https://esgf.nci.org.au/thredds/dodsC/master/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCESS-CM2/ssp585/r1i1p1f1/day/pr/gn/v20191108/pr_day_ACCESS-CM2_ssp585_r1i1p1f1_gn_20150101-20641231.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xarray + Dask\n",
    "\n",
    "Xarray can automatically wrap its data into Dask arrays. This capability turns Xarray into an extremely powerful tool when working with big earth science datasets. \n",
    "\n",
    "To see this in action, we will download a fairly large dataset to analyze. We use Xarray's `open_mfdataset` to allow multiple files to be opened simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On DKRZ system\n",
    "\n",
    "!ls /pool/data/CMIP6/data/ScenarioMIP/CSIRO-ARCCSS/ACCESS-CM2/ssp585/r1i1p1f1/day/pr/gn/v20191108\n",
    "path = '/pool/data/CMIP6/data/ScenarioMIP/CSIRO-ARCCSS/ACCESS-CM2/ssp585/r1i1p1f1/day/pr/gn/v20191108/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ssp585 = xr.open_mfdataset(path, combine='by_coords')\n",
    "f_ssp585"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>NOTE:</b> the values are not displayed, since that would trigger computation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks\n",
    "\n",
    "Notice that it says:`pr(time, lat, lon) float32 dask.array<chunksize=(18263, 144, 192), meta=np.ndarray>`. There is now the `chunksize` component. The data array also becomes a Dask array.\n",
    "\n",
    "The chunking of the array comes from the integration of Dask with Xarray. Dask divides the data array into small pieces called \"chunks\", with each chunk designed to be small enough to fit into memory. \n",
    "\n",
    "The file itself may be already chunked. Filesystem chunking is available in netCDF-4 and HDF5 datasets. The CMIP6 data should all be netCDF-4 and include some form of chunking for each file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the file metadata in the \"Data\" section above, we see in this case the file is chunked such that  \n",
    "#### `pr:_ChunkSizes = 1, 144, 192 ;`\n",
    "\n",
    "Here we see that the data is chunked in space but not time, where one chunk is one time-step and all points in lat-lon.\n",
    "\n",
    "![](chunks.png)\n",
    "image source: https://www.unidata.ucar.edu/blogs/developer/en/entry/chunking_data_why_it_matters\n",
    "\n",
    "Consider 2 types of data access\n",
    "1. Accessing a 2D lat-lon slice in time (RHS figure)\n",
    "2. Accessing a time series at a single lat-lon point (LHS figure)\n",
    "\n",
    "With the chunking above, the first type of data access only requires access to a single chunk, while the second type needs to access ALL the chunks of the data array regardless. This dataset will be fastest for 2D lat-lon single time-step data access.\n",
    "\n",
    "In general, even without chunking - when the data is accessed contiguously (by index order) - time is the slowest variable to access, then y, with x being the fastest. With the chunking method of this CMIP6 dataset, time still remains the slowest variable. More uniform variable access speeds would require more evenly spaced chunks.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Time how long it takes to load the precipitation data at `time='2015-01-01'` and then time how long it takes to load the data at `lat=0` and `lon=180` (remember to use `method='nearest'` for the latter case). How much difference in time is there when using these different access methods?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "f_ssp585.pr.sel(time='2015-01-01').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "f_ssp585.pr.sel(lat=0,lon=180,method='nearest').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same volume of data can take orders of magnitude longer to load\n",
    "\n",
    "The spatial dataset contained 27648 data-points and took in the order of 10ms to load. The time-series dataset had 31411 data-points and took order 20,000 ms to load.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>NOTE:</b> If you look at the dashboard, the task stream actually shows that the most time consuming part is data concatenation. \n",
    "</div>\n",
    "\n",
    "Chunking and the ways in which data is read is important when considering both how you access the data and if you wish to parallelise your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetCDF file Chunks versus Dask Chunks\n",
    "\n",
    "Keep in mind, Dask chunking is different to chunking of the stored data. As we saw in our example, the stored data was chunked with chunks of size (1,144,192) whereas the Dask array had a chunk size of (18263, 144, 192). It's possible to change the chunking size in the Dask array. In the example below, we are specifying that there are 730 chunks in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ssp585 = xr.open_mfdataset(path,chunks={'time':730}, combine='by_coords')\n",
    "\n",
    "f_ssp585"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big do you make your chunks?\n",
    "\n",
    "The rule of thumb for Dask chunks is that you should \"create arrays with a minimum chunksize of at least one million elements\":  http://xarray.pydata.org/en/stable/dask.html#chunking-and-performance\n",
    "\n",
    "NetCDF4 chunks are often a lot smaller than Dask array chunks. The minimum chunksize exists because if you have too many chunks, queuing of operations when parallelising will be slow. If the chunk sizes are too large, computation and memory can be wasted. The default chunks from dask gave us chunks of size (18263, 144, 192) or around 500 million elements so we could try reducing those chunks if needed. The larger the array, the larger the cost of queueing and therefore larger chunks may be needed.\n",
    "\n",
    "#### IMPORTANT: Whatever Dask array chunks you use, make sure they align with the netCDF4 file chunks!!\n",
    "\n",
    "So far our chunks have been in time, and the netCDF4 file is also chunked in time. If we tried to use dask chunks to optimise the time-series loading of data, it would not help! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Try to load the data in with chunks size `(31411,180,1)` (i.e. chunked in lon) and name that file `f_bad_chunk`. Next, try reloading the time series of pr at `lat=0` and `lon=180` and time how long this takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bad_chunk = xr.open_mfdataset(path,chunks={'time':31411,'lat':180,'lon':1}, combine='by_coords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running your previous code for `f_bad_chunk` again loading the time series of pr at `lat=0` and `lon=180` and time how long it takes if you scale up or down the number of workers. \n",
    "\n",
    "Do the same with the original chunking method of `f_ssp585` and see if there is a difference.\n",
    "\n",
    "We will use three different schedulers to read the data. First, let's initiate a client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Warning: Please make sure you specify the correct path to the scheduler.json file within your environment.</b>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the Dask Client will provide a dashboard which is useful to gain insight into the computation. The link to the dashboard will become visible when you create the Client. We recommend having the Client open on one side of your screen and your notebook open on the other side, which will be useful for learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "f_bad_chunk.pr.sel(lat=0,lon=180,method='nearest').load(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "f_bad_chunk.pr.sel(lat=0,lon=180,method='nearest').load(scheduler='threads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_bad_chunk.pr.sel(lat=0,lon=180,method='nearest').load(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poor chunking with dask can make your performance worse!\n",
    "\n",
    "As you can see, bad chunks and the alignment of the chunks slow down the I/O performance significantly. They are both  important to keep in mind when creating Dask chunks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the client\n",
    "\n",
    "Before moving on to the next exercise, make sure to close your client or stop this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This example shows how to make data chunking with Dask. \n",
    "\n",
    "For further information regarding Dask, please see: https://docs.dask.org/en/latest/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
